from collections import Counter

import pickle
import pandas as pd
import numpy as np
import joblib
import re
from pymorphy3 import MorphAnalyzer
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import ComplementNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.base import BaseEstimator, TransformerMixin

from configuration import STOPWORDS_RU, REQUIRED_COLUMNS, MIN_SAMPLES
from additional_functions import confusion_matrix_to_markdown, check_claras_folder
from custom_errors import RowCountError, ClassRepresentationError, ClassSampleSizeError, LoadModelError


class TextCleaner(BaseEstimator, TransformerMixin):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    def __init__(self, stop_words=None):
        self.stop_words = stop_words or set()
        self.morph = MorphAnalyzer()
    
    def lemmatize(self, word):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        if len(word) <= 2 or word.isupper():
            return word
        return self.morph.parse(word)[0].normal_form
    
    def lemmatize_word(self, word: str) -> str:
        return self.morph.parse(word)[0].normal_form

    def clean_text(self, text):
        text = str(text).lower()
        text = re.sub(r'\d+|[{}]'.format(re.escape('!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~¬´¬ª‚Äî')), ' ', text)
        text = ' '.join(text.split())
        
        # words = [self.lemmatize_word(word) for word in text.split() 
        #         if word not in self.stop_words]
        words = [word for word in text.split() if word not in self.stop_words]
        return ' '.join(words).strip()

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.astype(str).apply(self.clean_text)

class AssetClassifier:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º"""

    def __init__(self,
                 max_features=30000,
                 test_size=0.2,
                 random_state=42,
                 n_jobs=-1):
        self.max_features = max_features
        self.test_size = test_size
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.label_encoder = LabelEncoder()
        self.model = None
        self.stop_words_russian = STOPWORDS_RU  # –ü—Ä–∏–º–µ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤

    def _is_sample_suitable(self,
                            X,
                            y,
                            label_encoder=None,
                            min_samples=MIN_SAMPLES,
                            min_classes=2,
                            min_samples_per_class=10):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - X: –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–µ–∫—Å—Ç—ã)
        - y: –º–µ—Ç–∫–∏ (–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∏—Å–ª–∞)
        - label_encoder: —ç–∫–∑–µ–º–ø–ª—è—Ä LabelEncoder –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫
        - min_samples: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
        - min_classes: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        - min_samples_per_class: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - True, –µ—Å–ª–∏ –≤—ã–±–æ—Ä–∫–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç, –∏–Ω–∞—á–µ False
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
        if len(X) < min_samples:
            raise RowCountError(f"(!) –í—ã–±–æ—Ä–∫–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞ (—Å—Ç—Ä–æ–∫–∏={len(X)} < {min_samples}).")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        unique_classes, class_counts = np.unique(y, return_counts=True)
        if len(unique_classes) < min_classes:
            raise ClassRepresentationError(f"(!) –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–ª–∞—Å—Å–æ–≤ (n_classes={len(unique_classes)} < {min_classes}).")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Å–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        if np.any(class_counts < min_samples_per_class):
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω label_encoder)
            if label_encoder is not None:
                problematic_classes = {
                    label_encoder.inverse_transform([cls])[0]: int(count)
                    for cls, count in zip(unique_classes, class_counts)
                    if count < min_samples_per_class
                }
            else:
                problematic_classes = {
                    int(cls): int(count)
                    for cls, count in zip(unique_classes, class_counts)
                    if count < min_samples_per_class
                }
            raise ClassSampleSizeError(f"(!) –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∞—Å—Å—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {problematic_classes} < {min_samples_per_class}.")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –ø–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏
        vectorizer = TfidfVectorizer(min_df=1, max_df=1.0)
        try:
            X_vec = vectorizer.fit_transform(X)
            if X_vec.shape[1] == 0:
                print("(!) –ü–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å `min_df`/`max_df`.")
                return False
        except ValueError as e:
            print(f"(!) –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return False

        return True

    def train(self, df, text_column=REQUIRED_COLUMNS[0], target_column=REQUIRED_COLUMNS[1]):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
            target_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            df = df.dropna(subset=REQUIRED_COLUMNS, how='any')
            X = df[text_column].astype(str)
            y = self.label_encoder.fit_transform(df[target_column])

            if not self._is_sample_suitable(X, y, self.label_encoder):
                raise ValueError("–í—ã–±–æ—Ä–∫–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=self.test_size,
                random_state=self.random_state,
                stratify=y
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
            pipeline = make_pipeline(
                TextCleaner(stop_words=self.stop_words_russian),
                TfidfVectorizer(
                    max_features=self.max_features,
                    ngram_range=(1, 3),
                    sublinear_tf=True,
                    analyzer='word',
                    min_df=2,
                    max_df=0.9
                ),
                ComplementNB()
            )

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–¥–±–æ—Ä–∞
            param_dist = {
                'tfidfvectorizer__max_features': [15000, 20000],
                'tfidfvectorizer__ngram_range': [(1, 2), (1, 3)],
                'tfidfvectorizer__min_df': [2, 3, 5],
                'complementnb__alpha': np.logspace(-5, 0, 6),
                'complementnb__norm': [True, False],
            }

            # –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            search = RandomizedSearchCV(
                pipeline,
                param_dist,
                n_iter=20,
                cv=5,
                n_jobs=self.n_jobs,
                verbose=0,
                random_state=self.random_state,
                scoring='f1_weighted'
            )

            search.fit(X_train, y_train)
            self.model = search.best_estimator_

            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            report = self.generate_training_report(X_test, y_test)

            return report

        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
            raise


    def save_model(self, path_prefix=None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –≤ –ø–∞–ø–∫—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—É—é check_claras_folder"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        prefix = path_prefix or "asset_classifier"

        folder_path = check_claras_folder()

        model_filename = f"{prefix}_model_{timestamp}.joblib"
        full_path = folder_path / model_filename

        model_components = {
            'model': self.model,
            'vectorizer': self.label_encoder
        }

        joblib.dump(model_components, full_path)

        return str(full_path)

    @classmethod
    def load_model(cls, model_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            loaded = joblib.load(model_path)
        except (EOFError,
                pickle.UnpicklingError,
                ModuleNotFoundError,
                RuntimeError,
                pickle.PicklingError,
                MemoryError) as e:
            raise LoadModelError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {e}")

        classifier = cls()
        classifier.model = loaded['model']
        classifier.label_encoder = loaded['vectorizer']
        return classifier

    def predict(self, new_data, text_column=REQUIRED_COLUMNS[0], return_proba=False, output_file=None):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        Args:
            new_data: DataFrame –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
            text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º (–µ—Å–ª–∏ new_data - DataFrame)
            return_proba: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
            output_file: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        Returns:
            tuple:
                - DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                - str —Å markdown-—Ç–∞–±–ª–∏—Ü–µ–π –ø–µ—Ä–≤—ã—Ö 100 —Å—Ç—Ä–æ–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        import numpy as np

        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if isinstance(new_data, pd.DataFrame):
                result = new_data.copy()
                texts = result[text_column].astype(str)
            else:
                texts = pd.Series(new_data).astype(str)
                result = pd.DataFrame({text_column: texts})

            if return_proba:
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
                proba = self.model.predict_proba(texts)
                # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∏ –æ–∫—Ä—É–≥–ª—è–µ–º
                proba_percent = (proba * 100).round(0).astype(int)
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–∫ '%' –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º
                proba_percent_str = proba_percent.astype(str) + '%'
                proba_df = pd.DataFrame(proba_percent_str, columns=[f'–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å_{cls}' for cls in self.label_encoder.classes_])
                result = pd.concat([result, proba_df], axis=1)

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                max_idx = np.argmax(proba, axis=1)
                preds = max_idx
                result['–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è_–≥—Ä—É–ø–ø–∞'] = self.label_encoder.inverse_transform(preds)

                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
                probs_percent = proba_percent[np.arange(len(proba)), max_idx]
                result['–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å'] = probs_percent.astype(str) + '%'

                # –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–ø–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º)
                avg_proba = probs_percent.mean()
                avg_proba_str = f"{avg_proba:.1f}%"

            else:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                preds = self.model.predict(texts)
                result['–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è_–≥—Ä—É–ø–ø–∞'] = self.label_encoder.inverse_transform(preds)
                # –í –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å—Ç–∞–≤–∏–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
                result['–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å'] = ''
                avg_proba_str = 'N/A'

            # –§–æ—Ä–º–∏—Ä—É–µ–º markdown-—Ç–∞–±–ª–∏—Ü—É –ø–µ—Ä–≤—ã—Ö 100 —Å—Ç—Ä–æ–∫
            md_lines = []

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ —Å—Ä–µ–¥–Ω–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (–µ—Å–ª–∏ –µ—Å—Ç—å)
            md_lines.append(f"**üìä –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** {avg_proba_str}\n")

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            md_lines.append("**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**")
            md_lines.append("> **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >80%** - –≤—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å\n")
            md_lines.append("> **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 60-80%** - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞\n")
            md_lines.append("> **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å <60%** - –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n")

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
            header = f"| {text_column.capitalize()} | –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è –≥—Ä—É–ø–ø–∞ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å |"
            separator = f"|{'-' * (len(text_column)+2)}|----------------------|-------------|"
            md_lines.append(header)
            md_lines.append(separator)

            for _, row in result.head(100).iterrows():
                name = str(row[text_column])
                pred = str(row['–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è_–≥—Ä—É–ø–ø–∞'])
                prob = str(row['–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å'])
                md_lines.append(f"| {name} | {pred} | {prob} |")

            markdown_table = "\n".join(md_lines)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            # claras_folder = check_claras_folder()
            if output_file:
                # # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                # if not output_file.endswith('.xlsx'):
                #     output_file += '.xlsx'
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                # output_path = claras_folder / output_file
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                result.to_excel(output_file, index=False)
                print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

            return result, markdown_table

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {str(e)}")
            raise




    def generate_training_report(self, X_test, y_test):
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∏—Ç–æ–≥–∞–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏, confusion matrix –∏ classification report.
        –ü—Ä–∏ —É–∫–∞–∑–∞–Ω–∏–∏ output_pdf_path —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –∏ –≥—Ä–∞—Ñ–∏–∫ –≤ PDF.

        Args:
            X_test (pd.Series –∏–ª–∏ —Å–ø–∏—Å–æ–∫): –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã.
            y_test (array-like): –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (—á–∏—Å–ª–æ–≤—ã–µ, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LabelEncoder).

        Returns:
            str: –û—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ markdown.
        """


        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = self.model.predict(X_test)

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        y_test_decoded = self.label_encoder.inverse_transform(y_test)
        y_pred_decoded = self.label_encoder.inverse_transform(y_pred)

        # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        accuracy = np.mean(y_pred == y_test)
        accuracy_pct = int(round(accuracy * 100))

        # –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if hasattr(self.model, 'predict_proba'):
            proba = self.model.predict_proba(X_test)
            correct_class_indices = [list(self.label_encoder.classes_).index(label) for label in y_test_decoded]
            correct_probas = [proba[i, idx] for i, idx in enumerate(correct_class_indices)]
            avg_correct_proba = int(round(np.mean(correct_probas) * 100))
        else:
            avg_correct_proba = None

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥—Ä—É–ø–ø–∞–º (–∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏)
        group_counts = Counter(y_test_decoded)
        total = len(y_test_decoded)

        # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º
        group_report = {}
        for group in self.label_encoder.classes_:
            total_group = group_counts.get(group, 0)
            idxs = [i for i, val in enumerate(y_test_decoded) if val == group]
            if total_group == 0:
                continue
            correct = sum(1 for i in idxs if y_pred_decoded[i] == group)
            errors = [y_pred_decoded[i] for i in idxs if y_pred_decoded[i] != group]
            error_counts = Counter(errors)
            common_errors = error_counts.most_common(3)
            group_report[group] = {
                'total': total_group,
                'correct': correct,
                'accuracy_pct': int(round(correct / total_group * 100)),
                'common_errors': common_errors
            }

        # –§–æ—Ä–º–∏—Ä—É–µ–º markdown –æ—Ç—á–µ—Ç
        lines = []
        lines.append("### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
        lines.append(f"**üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** {accuracy_pct}%")
        if avg_correct_proba is not None:
            lines.append(f"\n**üìä –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** {avg_correct_proba}%")
        
        lines.append("\n### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥—Ä—É–ø–ø–∞–º")
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –Ω–æ–≤—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏
        lines.append("| –ì—Ä—É–ø–ø–∞ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –î–æ–ª—è | –¢–æ—á–Ω–æ—Å—Ç—å | –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è |")
        lines.append("|---|---|---|---|---|")
        
        total = len(y_test_decoded)
        
        for group, count in group_counts.most_common():
            pct = int(round(count / total * 100))
            accuracy_group = group_report[group]['accuracy_pct'] if group in group_report else 0
        
            # –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–µ
            if avg_correct_proba is not None and hasattr(self.model, 'predict_proba'):
                # –ò–Ω–¥–µ–∫—Å—ã –æ–±—ä–µ–∫—Ç–æ–≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
                idxs = [i for i, val in enumerate(y_test_decoded) if val == group]
                # –ò–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
                class_idx = list(self.label_encoder.classes_).index(group)
                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –≥—Ä—É–ø–ø—ã
                group_probas = [proba[i, class_idx] for i in idxs]
                avg_group_proba = int(round(np.mean(group_probas) * 100)) if group_probas else 0
            else:
                avg_group_proba = "N/A"
        
            lines.append(f"| {group} | {count} | {pct}% | {accuracy_group}% | {avg_group_proba}% |")


        # Confusion matrix
        classes = self.label_encoder.classes_
        cm = confusion_matrix(y_test_decoded, y_pred_decoded, labels=classes)

        md_table = confusion_matrix_to_markdown(cm, classes)

        lines.append("### –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n\n" + md_table + "\n")
        
        lines.append("\n### –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º")
        for group, stats in group_report.items():
            lines.append(f"1. **{group}**")
            lines.append(f"   - –í–µ—Ä–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {stats['correct']} –∏–∑ {stats['total']} ({stats['accuracy_pct']}%)")
            if stats['common_errors']:
                error_strs = [f'"{err[0]}" ({err[1]})' for err in stats['common_errors']]
                lines.append(f"   - –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏: –ø—É—Ç–∞–µ—Ç —Å {', '.join(error_strs)}")
            else:
                lines.append("   - –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            lines.append("")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        lines.append("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
        lines.append("–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏:\n")
        lines.append("‚úÖ –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Å–ª–∞–±–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø\n")
        lines.append("‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏ —É—Ç–æ—á–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≥—Ä—É–ø–ø —Å —á–∞—Å—Ç—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏\n")
        lines.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –æ—Ç 10 —Ç—ã—Å.\n")

        report_text = "\n".join(lines)

        return report_text
